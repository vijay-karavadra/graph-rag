{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqhMgAGwmLXc"
   },
   "source": [
    "# LazyGraphRAG in LangChain\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In [LazyGraphRAG](https://www.microsoft.com/en-us/research/blog/lazygraphrag-setting-a-new-standard-for-quality-and-cost/), Microsoft demonstrates significant cost and performance benefits to delaying the construction of a knowledge graph.\n",
    "This is largely because not all documents need to be analyzed.\n",
    "However, it is also benefical that documents by the time documents are analyzed the question is already known, allowing irrelevant information to be ignored. \n",
    "\n",
    "We've noticed similar cost benefits to building a document graph linking content based on simple properties such as extracted keywords compared to building a complete knowledge graph.\n",
    "For the Wikipedia dataset used in this notebook, we estimated it would have taken $70k to build a knowledege graph using the [example from LangChain](https://python.langchain.com/docs/how_to/graph_constructing/#llm-graph-transformer), while the document graph was basically free.\n",
    "\n",
    "In this notebook we demonstrate how to populate a document graph with Wikipedia articles linked based on mentions in the articles and extracted keywords.\n",
    "Keyword extraction uses a local [KeyBERT](https://maartengr.github.io/KeyBERT/) model, making it fast and cost-effective to construct these graphs.\n",
    "We'll then show how to build out a chain which does the steps of Lazy GraphRAG -- retrieving articles, extracting claims from each community, ranking and selecting the top claims, and generating an answer based on those claims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6dFtwI_xmFW"
   },
   "source": [
    "## Environment Setup\n",
    "\n",
    "The following block will configure the environment from the Colab Secrets.\n",
    "To run it, you should have the following Colab Secrets defined and accessible to this notebook:\n",
    "\n",
    "- `OPENAI_API_KEY`: The OpenAI key.\n",
    "- `ASTRA_DB_API_ENDPOINT`: The Astra DB API endpoint.\n",
    "- `ASTRA_DB_APPLICATION_TOKEN`: The Astra DB Application token.\n",
    "- `LANGCHAIN_API_KEY`: Optional. If defined, will enable LangSmith tracing.\n",
    "- `ASTRA_DB_KEYSPACE`: Optional. If defined, will specify the Astra DB keyspace. If not defined, will use the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "r0-5VJWGsBM3",
    "tags": [
     "hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "# Install modules.\n",
    "#\n",
    "# On Apple hardware, \"spacy[apple]\" will improve performance.\n",
    "%pip install \\\n",
    "    langchain-core \\\n",
    "    langchain-astradb \\\n",
    "    langchain-openai \\\n",
    "    langchain-graph-retriever \\\n",
    "    spacy \\\n",
    "    graph-rag-example-helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last package -- `graph-rag-example-helpers` -- includes some helpers for setting up environment helpers and allowing the loading of wikipedia data to be restarted if it fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "# Downloads the model used by Spacy for extracting entities.\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "# Configure import paths.\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "# Initialize environment variables.\n",
    "from graph_rag_example_helpers.env import Environment, initialize_environment\n",
    "\n",
    "initialize_environment(Environment.ASTRAPY)\n",
    "\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"lazy-graph-rag\"\n",
    "\n",
    "# The full dataset is ~6m documents, and takes hours to load.\n",
    "# The short dataset is 1000 documents and loads quickly.\n",
    "# Change this to `True` to use the larger dataset.\n",
    "USE_SHORT_DATASET = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading Data\n",
    "\n",
    "First, we'll demonstrate how to load Wikipedia data into an `AstraDBVectorStore`, using the mentioned articles and keywords as metadata fields.\n",
    "In this section, we're not actually doing anything special for the graph -- we're just populating the metadata with fields that useful describe our content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Documents from Wikipedia Articles\n",
    "The first thing we need to do is create the `LangChain` `Document`s we'll import.\n",
    "\n",
    "To do this, we write some code to convert lines from a JSON file downloaded from [2wikimultihop](https://github.com/Alab-NII/2wikimultihop?tab=readme-ov-file#new-update-april-7-2021) and create a `Document`.\n",
    "We populate the `id` and `metadata[\"mentions\"]` from information in this file.\n",
    "\n",
    "Then, we run those documents through the `SpacyNERTransformer` to populate `metadata[\"entities\"]` with entities named in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8u4lD-AqDMMs"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections.abc import Iterator\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_graph_retriever.transformers.spacy import (\n",
    "    SpacyNERTransformer,\n",
    ")\n",
    "\n",
    "\n",
    "def parse_document(line: bytes) -> Document:\n",
    "    \"\"\"Reads one JSON line from the wikimultihop dump.\"\"\"\n",
    "    para = json.loads(line)\n",
    "\n",
    "    id = para[\"id\"]\n",
    "    title = para[\"title\"]\n",
    "\n",
    "    # Use structured information (mentioned Wikipedia IDs) as metadata.\n",
    "    mentioned_ids = [id for m in para[\"mentions\"] for m in m[\"ref_ids\"] or []]\n",
    "\n",
    "    return Document(\n",
    "        id=id,\n",
    "        page_content=\" \".join(para[\"sentences\"]),\n",
    "        metadata={\n",
    "            \"mentions\": mentioned_ids,\n",
    "            \"title\": title,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "NER_TRANSFORMER = SpacyNERTransformer(\n",
    "    limit=1000,\n",
    "    exclude_labels={\"CARDINAL\", \"MONEY\", \"QUANTITY\", \"TIME\", \"PERCENT\", \"ORDINAL\"},\n",
    ")\n",
    "\n",
    "\n",
    "# Load data in batches, using GLiNER to extract entities.\n",
    "def prepare_batch(lines: Iterator[str]) -> Iterator[Document]:\n",
    "    # Parse documents from the batch of lines.\n",
    "    docs = [parse_document(line) for line in lines]\n",
    "\n",
    "    docs = NER_TRANSFORMER.transform_documents(docs)\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the AstraDBVectorStore\n",
    "Next, we create the Vector Store we're going to load these documents into.\n",
    "In our case, we use DataStax Astra DB with Open AI embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_astradb import AstraDBVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "COLLECTION = \"lazy_graph_rag_short\" if USE_SHORT_DATASET else \"lazy_graph_rag\"\n",
    "store = AstraDBVectorStore(\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    collection_name=COLLECTION,\n",
    "    pre_delete_collection=USE_SHORT_DATASET,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data into the Store\n",
    "Next, we perform the actual loading.\n",
    "This takes a while, so we use a helper utility to persist which batches have been written so we can resume if there are any failures.\n",
    "\n",
    "On OS X, it is useful to run `caffeinate -dis` in a shell to prevent the machine from going to sleep and seems to reduce errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "\n",
    "from graph_rag_example_helpers.datasets.wikimultihop import aload_2wikimultihop\n",
    "\n",
    "# Path to the file `para_with_hyperlink.zip`.\n",
    "# See instructions here to download from\n",
    "# [2wikimultihop](https://github.com/Alab-NII/2wikimultihop?tab=readme-ov-file#new-update-april-7-2021).\n",
    "PARA_WITH_HYPERLINK_ZIP = os.path.join(os.getcwd(), \"para_with_hyperlink.zip\")\n",
    "\n",
    "await aload_2wikimultihop(\n",
    "    limit=100 if USE_SHORT_DATASET else None,\n",
    "    full_para_with_hyperlink_zip_path=PARA_WITH_HYPERLINK_ZIP,\n",
    "    store=store,\n",
    "    batch_prepare=prepare_batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we've created a `VectorStore` with the Wikipedia articles.\n",
    "Each article is associated with metadata identifying other articles it mentions and entities from the article.\n",
    "\n",
    "As is, this is useful for performing a vector search filtered to articles mentioning a specific term or performing an entity seach on the documents.\n",
    "The library `langchain-graph-retriever` makes this even more useful by allowing articles to be traversed based on relationships such as articles mentioned in the current article (or mentioning the current article) or articles providing more information on the entities mentioned in the current article.\n",
    "\n",
    "In the next section we'll see not just how we can use the relationships in the metadata to retrieve more articles, but we'll go a step further and perform Lazy GraphRAG to extract relevant claims from both the similar and related articles and use the most relevant claims to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Lazy Graph RAG via Hierarchical Summarization\n",
    "\n",
    "As we've noted before, eagerly building a knowledge graph is prohibitively expensive.\n",
    "Microsoft seems to agree, and recently introduced LazyGraphRAG, which enables GraphRAG to be performed late -- after a query is retrieved.\n",
    "\n",
    "We implement the LazyGraphRAG technique using the traversing retrievers as follows:\n",
    "\n",
    "1. Retrieve a good number of nodes using a traversing retrieval.\n",
    "2. Identify communities in the retrieved sub-graph.\n",
    "3. Extract claims from each community relevant to the query using an LLM.\n",
    "4. Rank each of the claims based on the relevance to the question and select the top claims.\n",
    "5. Generate an answer to the question based on the extracted claims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain for Extracting Claims\n",
    "\n",
    "The first thing we do is create a chain that produces the claims. Given an input containing the question and the retrieved communities, it applies an LLM in parallel extracting claims from each community.\n",
    "\n",
    "A claim is just a string representing the statement and the `source_id` of the document. We request structured output so we get a list of claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "from operator import itemgetter\n",
    "from typing import TypedDict\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel, chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Claim(BaseModel):\n",
    "    \"\"\"Representation of an individual claim from a source document(s).\"\"\"\n",
    "\n",
    "    claim: str = Field(description=\"The claim from the original document(s).\")\n",
    "    source_id: str = Field(description=\"Document ID containing the claim.\")\n",
    "\n",
    "\n",
    "class Claims(BaseModel):\n",
    "    \"\"\"Claims extracted from a set of source document(s).\"\"\"\n",
    "\n",
    "    claims: list[Claim] = Field(description=\"The extracted claims.\")\n",
    "\n",
    "\n",
    "MODEL = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "CLAIMS_MODEL = MODEL.with_structured_output(Claims)\n",
    "\n",
    "CLAIMS_PROMPT = ChatPromptTemplate.from_template(\"\"\"\n",
    "Extract claims from the following related documents.\n",
    "\n",
    "Only return claims appearing within the specified documents.\n",
    "If no documents are provided, do not make up claims or documents.\n",
    "\n",
    "Claims (and scores) should be relevant to the question.\n",
    "Don't include claims from the documents if they are not directly or indirectly\n",
    "relevant to the question.\n",
    "\n",
    "If none of the documents make any claims relevant to the question, return an\n",
    "empty list of claims.\n",
    "\n",
    "If multiple documents make similar claims, include the original text of each as\n",
    "separate claims. Score the most useful and authoritative claim higher than\n",
    "similar, lower-quality claims.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "{formatted_documents}\n",
    "\"\"\")\n",
    "\n",
    "# TODO: Few-shot examples? Possibly with a selector?\n",
    "\n",
    "\n",
    "def format_documents_with_ids(documents: Iterable[Document]) -> str:\n",
    "    formatted_docs = \"\\n\\n\".join(\n",
    "        f\"Document ID: {doc.id}\\nContent: {doc.page_content}\" for doc in documents\n",
    "    )\n",
    "    return formatted_docs\n",
    "\n",
    "\n",
    "CLAIM_CHAIN = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "            \"formatted_documents\": itemgetter(\"documents\")\n",
    "            | RunnableLambda(format_documents_with_ids),\n",
    "        }\n",
    "    )\n",
    "    | CLAIMS_PROMPT\n",
    "    | CLAIMS_MODEL\n",
    ")\n",
    "\n",
    "\n",
    "class ClaimsChainInput(TypedDict):\n",
    "    question: str\n",
    "    communities: Iterable[Iterable[Document]]\n",
    "\n",
    "\n",
    "@chain\n",
    "async def claims_chain(input: ClaimsChainInput) -> Iterable[Claim]:\n",
    "    question = input[\"question\"]\n",
    "    communities = input[\"communities\"]\n",
    "\n",
    "    # TODO: Use openai directly so this can use the batch API for performance/cost?\n",
    "    community_claims = await CLAIM_CHAIN.abatch(\n",
    "        [{\"question\": question, \"documents\": community} for community in communities]\n",
    "    )\n",
    "    return [claim for community in community_claims for claim in community.claims]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain for Ranking Claims\n",
    "\n",
    "The next chain is used for ranking the claims so we can select the most relevant to the question.\n",
    "\n",
    "This is based on ideas from [RankRAG](https://arxiv.org/abs/2407.02485).\n",
    "Specifically, the prompt is constructed so that the next token should be `True` if the content is relevant and `False` if not.\n",
    "The probability of the token is used to determine the relevance -- `True` with a higher probability is more relevant than `True` with a lesser probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "RANK_PROMPT = ChatPromptTemplate.from_template(\"\"\"\n",
    "Rank the relevance of the following claim to the question.\n",
    "Output \"True\" if the claim is relevant and \"False\" if it is not.\n",
    "Only output True or False.\n",
    "\n",
    "Question: Where is Seattle?\n",
    "\n",
    "Claim: Seattle is in Washington State.\n",
    "\n",
    "Relevant: True\n",
    "\n",
    "Question: Where is LA?\n",
    "\n",
    "Claim: New York City is in New York State.\n",
    "\n",
    "Relevant: False\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Claim: {claim}\n",
    "\n",
    "Relevant:\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def compute_rank(msg):\n",
    "    logprob = msg.response_metadata[\"logprobs\"][\"content\"][0]\n",
    "    prob = math.exp(logprob[\"logprob\"])\n",
    "    token = logprob[\"token\"]\n",
    "    if token == \"True\":\n",
    "        return prob\n",
    "    elif token == \"False\":\n",
    "        return 1.0 - prob\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected logprob: {logprob}\")\n",
    "\n",
    "\n",
    "RANK_CHAIN = RANK_PROMPT | MODEL.bind(logprobs=True) | RunnableLambda(compute_rank)\n",
    "\n",
    "\n",
    "class RankChainInput(TypedDict):\n",
    "    question: str\n",
    "    claims: Iterable[Claim]\n",
    "\n",
    "\n",
    "@chain\n",
    "async def rank_chain(input: RankChainInput) -> Iterable[Claim]:\n",
    "    # TODO: Use openai directly so this can use the batch API for performance/cost?\n",
    "    claims = input[\"claims\"]\n",
    "    ranks = await RANK_CHAIN.abatch(\n",
    "        [{\"question\": input[\"question\"], \"claim\": claim} for claim in claims]\n",
    "    )\n",
    "    rank_claims = sorted(\n",
    "        zip(ranks, claims, strict=True), key=lambda rank_claim: rank_claim[0]\n",
    "    )\n",
    "\n",
    "    return [claim for _, claim in rank_claims]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could extend this by using an MMR-like strategy for selecting claims.\n",
    "Specifically, we could combine the relevance of the claim to the question and the diversity compared to already selected claims to select the best variety of claims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LazyGraphRAG in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we produce a chain that puts everything together.\n",
    "Given a `GraphRetriever` it retrieves documents, creates communities using edges amongst the retrieved documents, extracts claims from those communities, ranks and selects the best claims, and then answers the question using those claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from graph_retriever.edges import EdgeSpec, MetadataEdgeFunction\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_graph_retriever import GraphRetriever\n",
    "from langchain_graph_retriever.document_graph import create_graph, group_by_community\n",
    "\n",
    "\n",
    "@chain\n",
    "async def lazy_graph_rag(\n",
    "    question: str,\n",
    "    *,\n",
    "    retriever: GraphRetriever,\n",
    "    model: BaseLanguageModel,\n",
    "    edges: Iterable[EdgeSpec] | MetadataEdgeFunction | None = None,\n",
    "    max_tokens: int = 1000,\n",
    "    **kwargs: Any,\n",
    ") -> str:\n",
    "    \"\"\"Retrieve claims relating to the question using LazyGraphRAG.\n",
    "\n",
    "    Returns the top claims up to the given `max_tokens` as a markdown list.\n",
    "\n",
    "    \"\"\"\n",
    "    edges = edges or retriever.edges\n",
    "    if edges is None:\n",
    "        raise ValueError(\"Must specify 'edges' in invocation or retriever\")\n",
    "\n",
    "    # 1. Retrieve documents using the (traversing) retriever.\n",
    "    documents = await retriever.ainvoke(question, edges=edges, **kwargs)\n",
    "\n",
    "    # 2. Create a graph and extract communities.\n",
    "    document_graph = create_graph(documents, edges=edges)\n",
    "    communities = group_by_community(document_graph)\n",
    "\n",
    "    # 3. Extract claims from the communities.\n",
    "    claims = await claims_chain.ainvoke(\n",
    "        {\"question\": question, \"communities\": communities}\n",
    "    )\n",
    "\n",
    "    # 4. Rank the claims and select claims up to the given token limit.\n",
    "    result_claims = []\n",
    "    tokens = 0\n",
    "\n",
    "    for claim in await rank_chain.ainvoke({\"question\": question, \"claims\": claims}):\n",
    "        claim_str = f\"- {claim.claim} (Source: {claim.source_id})\"\n",
    "\n",
    "        tokens += model.get_num_tokens(claim_str)\n",
    "        if tokens > max_tokens:\n",
    "            break\n",
    "        result_claims.append(claim_str)\n",
    "\n",
    "    return \"\\n\".join(result_claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Lazy GraphRAG in LangChain\n",
    "\n",
    "Finally, we sue the Lazy GraphRAG chain we created on the store we populated earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_retriever.edges import Id\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_graph_retriever import GraphRetriever\n",
    "\n",
    "RETRIEVER = GraphRetriever(\n",
    "    store=store,\n",
    "    edges=[(\"mentions\", Id()), (\"entities\", \"entities\")],\n",
    "    k=100,\n",
    "    start_k=30,\n",
    "    adjacent_k=20,\n",
    "    max_depth=3,\n",
    ")\n",
    "\n",
    "ANSWER_PROMPT = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the supporting claims.\n",
    "\n",
    "Only use information from the claims. Do not guess or make up any information.\n",
    "\n",
    "Where possible, reference and quote the supporting claims.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Claims:\n",
    "{claims}\n",
    "\"\"\")\n",
    "\n",
    "LAZY_GRAPH_RAG_CHAIN = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"claims\": RunnablePassthrough()\n",
    "        | lazy_graph_rag.bind(\n",
    "            retriever=RETRIEVER,\n",
    "            model=MODEL,\n",
    "            max_tokens=1000,\n",
    "        ),\n",
    "    }\n",
    "    | ANSWER_PROMPT\n",
    "    | MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "keep_output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bermudan sloop ships are widely prized for several reasons. Firstly, they feature the Bermuda rig, which is popular because it is easier to sail with a smaller crew or even single-handed, is cheaper due to having less hardware, and performs well when sailing into the wind (Source: 48520). Additionally, Bermuda sloops were constructed using Bermuda cedar, a material valued for its durability and resistance to rot, contributing to the ships' longevity and performance (Source: 17186373). These factors combined make Bermudan sloops highly valued compared to other ships.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUESTION = \"Why are Bermudan sloop ships widely prized compared to other ships?\"\n",
    "result = await LAZY_GRAPH_RAG_CHAIN.ainvoke(QUESTION)\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, below are the results to the same question using a basic RAG pattern with just vector similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "keep_output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The documents do not provide specific reasons why Bermudan sloop ships are widely prized compared to other ships. They describe the development and characteristics of the Bermuda sloop, such as its fore-and-aft rigged single-masted design and the use of the Bermuda rig with triangular sails, but do not explicitly state why these ships are particularly valued over others.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "VECTOR_ANSWER_PROMPT = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the provided documents.\n",
    "\n",
    "Only use information from the documents. Do not guess or make up any information.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Documents:\n",
    "{documents}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "VECTOR_CHAIN = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"documents\": (store.as_retriever() | format_docs),\n",
    "    }\n",
    "    | VECTOR_ANSWER_PROMPT\n",
    "    | MODEL\n",
    ")\n",
    "\n",
    "result = VECTOR_CHAIN.invoke(QUESTION)\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LazyGraphRAG chain is great when a question needs to consider a large amount of relevant information in order to produce a thorough answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This post demonstrated how easy it is to implement Lazy GraphRAG on top of a document graph.\n",
    "\n",
    "It used `langchain-graph-retriever` from the [graph-rag project](datastax.github.io/graph-rag) to implement the document graph and graph-based retrieval on top of an existing LangChain `VectorStore`.\n",
    "This means you can focus on populating and using your `VectorStore` with useful metadata and add graph-based retrieval and even Lazy GraphRAG when you need it.\n",
    "\n",
    "**Any LangChain `VectorStore` can be used with Lazy GraphRAG without needing to change or re-ingest the stored documents.**\n",
    "Knowledge Graphs and GraphRAG shouldn't be hard or scary.\n",
    "Start simple and easily overlay edges when you need them.\n",
    "\n",
    "Graph retrievers and LazyGraph RAG work well with agents.\n",
    "You can allow the agent to retrieve differently depending on the question -- doing a vector only search for simple questions, traversing to mentioned articles for a deeper question or traversing to articles that cite this to see if there is newer information available.\n",
    "We'll show how to combine these techniques with agents in a future post.\n",
    "Until then, give `langchain-graph-retriever` a try and let us know how it goes!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
